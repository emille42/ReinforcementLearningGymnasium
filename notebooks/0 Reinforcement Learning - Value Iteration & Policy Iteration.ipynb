{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOwuQAr46tM_"
      },
      "outputs": [],
      "source": [
        "#!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mWoAaX8u7KWb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import time\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1okNQdp2Gz-N"
      },
      "source": [
        "# Gymnasium - library exploration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5xNbTG7vl_"
      },
      "source": [
        "### Taxi-V3\n",
        "###Действия:\n",
        "\n",
        "- 0: двигаться вниз\n",
        "- 1: двигаться вверх\n",
        "- 2: двигаться вправо\n",
        "- 3: двигаться влево\n",
        "- 4: подобрать пассажира\n",
        "- 5: высадить пассажира\n",
        "---\n",
        "\n",
        "###Состояния:\n",
        "- 500 дискретных состояний: 25 позиций такси, 5 возможных локаций пассажира, включая пассажира в такси, и 4 локации назначения\n",
        "- за эпизод может быть достигнуто 400 состояний\n",
        "---\n",
        "\n",
        "###Локации пассажира:\n",
        "- 0: RED (R)\n",
        "- 1: GREEN (G)\n",
        "- 2: YELLOW (Y)\n",
        "- 3: BLUE (B)\n",
        "- 4: в такси\n",
        "---\n",
        "\n",
        "###Точки назначения:\n",
        "- 0: RED (R)\n",
        "- 1: GREEN (G)\n",
        "- 2: YELLOW (Y)\n",
        "- 3: BLUE (B)\n",
        "\n",
        "---\n",
        "###Награды:\n",
        "\n",
        "- -1 за каждое действие не дающее другую награду\n",
        "- +20 за правильную высадку пассажира\n",
        "- -10 за неправильную посадку/высадку пассажира\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRApftQi7TUN",
        "outputId": "766ee7ee-cc79-4a2a-bed9-2a8686b20293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\n",
        "observation, info = env.reset(seed=SEED)\n",
        "# - такси (желтое) - стартует в случайном месте\n",
        "# - пассажир - стартует в одной из 4 локаций высадки (R, G, B, Y)\n",
        "# - пассажир - буква выделена жирным, синим цветом\n",
        "# - когда пассажир подобран такси становится зеленым\n",
        "# - нужно доставить пассажира в указанную локацию, выделена фиолетовым\n",
        "print(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBgTcmvLF8ve",
        "outputId": "43dc6545-3dc5-4803-8195-dc04293ea8dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "386"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# номер текущего состояния\n",
        "observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOkIMxvEESVL",
        "outputId": "581aec7a-4383-4da9-bcb6-4d1531665d51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Размер пространства действий\n",
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue5f98U1EN4F",
        "outputId": "d5b44677-24c6-41f1-e8d4-6502c0cdfc1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Количество состояний среды\n",
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw7tI1L2EXIW",
        "outputId": "9961f903-cc53-476b-9b56-85bcaaacdc5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: [(1.0, 486, -1, False)],\n",
              " 1: [(1.0, 286, -1, False)],\n",
              " 2: [(1.0, 386, -1, False)],\n",
              " 3: [(1.0, 366, -1, False)],\n",
              " 4: [(1.0, 386, -10, False)],\n",
              " 5: [(1.0, 386, -10, False)]}"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# P - многомерный список, хранящий информацию обо всех состояниях и обо всех возмножных действиях\n",
        "# Нпример, возможные действия в состоянии 386:\n",
        "# Для каждого действия определены:\n",
        "#  -верятность перехода в указанное следующее состояние при выполнении действия\n",
        "#  -следующее состояние\n",
        "#  -награда за действие\n",
        "#  -находимся ли в терминальном состоянии\n",
        "env.P[386]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZCq3vNaFa_P",
        "outputId": "da9b68e7-17a8-4149-f78d-29177d3f260b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "Перешли в состояние: 366\n",
            "Получили награду за действие: -1\n"
          ]
        }
      ],
      "source": [
        "# Двигаем такси на запад\n",
        "observation, reward, terminated, truncated, info = env.step(3)\n",
        "print(env.render())\n",
        "print(f\"Перешли в состояние: {observation}\")\n",
        "print(f\"Получили награду за действие: {reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce6_4VUMJtm9",
        "outputId": "b935ab8a-b691-4e67-ff81-1485488fba72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)}"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# action_mask - показывает список возможных действий\n",
        "# из состояния 366 - это вверх, вниз, вправо\n",
        "info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsBgKSoBHhah",
        "outputId": "b459e446-1240-4a95-c3d7-4f2bd125715e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Current state: 410\n",
            "Truncated: False\n",
            "Terminated: True\n",
            "Total rewards: 8\n"
          ]
        }
      ],
      "source": [
        "# Визуализация одного эпизода\n",
        "actions = [1, 1, 1, 4, 3, 3, 0, 0, 3, 3, 0, 0, 5]\n",
        "reward_sum = 0\n",
        "env.reset(seed=SEED)\n",
        "print(env.render())\n",
        "\n",
        "for action_n in actions:\n",
        "  time.sleep(0.9)\n",
        "  clear_output(True)\n",
        "  observation, reward, terminated, truncated, info = env.step(action_n)\n",
        "  reward_sum +=reward\n",
        "  print(env.render())\n",
        "  print(f\"Current state: {observation}\")\n",
        "  print(f\"Truncated: {truncated}\")\n",
        "  print(f\"Terminated: {terminated}\")\n",
        "\n",
        "print(f\"Total rewards: {reward_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAtPHFaWLwIt",
        "outputId": "2a75e304-d0bf-4e60-bb96-cb3675d035f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: [(1.0, 418, -1, False)],\n",
              " 1: [(1.0, 318, -1, False)],\n",
              " 2: [(1.0, 418, -1, False)],\n",
              " 3: [(1.0, 418, -1, False)],\n",
              " 4: [(1.0, 418, -10, False)],\n",
              " 5: [(1.0, 410, 20, True)]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# При переходе из состояния в 418 в состояние 410 мы перейдем в терминальное состояние и получим награду за высадку пассажира\n",
        "env.P[418]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ccVh1selxBCn"
      },
      "source": [
        "# Value Iteration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LzuSxRaA8_vB"
      },
      "source": [
        "Изучим и реализуем алгоритм value iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PKW6D_LZaY3p"
      },
      "outputs": [],
      "source": [
        "def value_iteration(environment, gamma=1.0, theta=0.001, max_iter=100):\n",
        "  # V - хранит ценности состояний (state-value func)\n",
        "  V = np.zeros(environment.observation_space.n)\n",
        "\n",
        "  delta = 0.\n",
        "\n",
        "  for k in range(max_iter):\n",
        "    for state_n in range(environment.observation_space.n):\n",
        "\n",
        "      # Нам нужно посчитать награду за каждое действие в текущем состоянии\n",
        "      action_values = np.zeros(environment.action_space.n)\n",
        "      for action_n in range(environment.action_space.n):\n",
        "        # reward - немедленная награда за переход в новое состояние при действии action_n\n",
        "        # probability - вероятность перейти в следующее состояние при выполнении действия action_n\n",
        "        for probability, next_state, reward, terminated in environment.P[state_n][action_n]:\n",
        "        # Выполняем подсчет награды за действие action_n при переходе в следующее состояние next_state\n",
        "          action_values[action_n] += probability * (reward + gamma* V[next_state])\n",
        "\n",
        "      # Q_value - награда за действие a в состоянии s при следовании оптимальной политике pi\n",
        "      Q_value = np.max(action_values)\n",
        "\n",
        "      delta = max(delta, np.abs(V[state_n] - Q_value))\n",
        "\n",
        "      V[state_n] = Q_value\n",
        "\n",
        "    if delta < theta:\n",
        "        print(f\"Вычисление ценности для каждого состояния завершено на итерации: {k} \")\n",
        "        break\n",
        "\n",
        "  # policy - хранит лучшее действие для каждого состояния\n",
        "  policy = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
        "  for state_n in range(environment.observation_space.n ):\n",
        "\n",
        "      # Также считаем ценность действий\n",
        "      action_values = np.zeros(environment.action_space.n)\n",
        "      for action_n in range(environment.action_space.n):\n",
        "        for probability, next_state, reward, terminated in environment.P[state_n][action_n]:\n",
        "          action_values[action_n] += probability * (reward + gamma* V[next_state])\n",
        "\n",
        "      # Сохраняем индекс лучшего действия для текущего состояния\n",
        "      best_policy = np.argmax(action_values)\n",
        "      policy[state_n, best_policy] = 1\n",
        "\n",
        "  return policy, V"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIoIeX79F37"
      },
      "source": [
        "Обучим алгоритм Value Iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "magMtddt0YMv"
      },
      "outputs": [],
      "source": [
        "environment = gym.make(\"Taxi-v3\")\n",
        "policy, V = value_iteration(environment, max_iter=1000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Krgl9kc9NxB"
      },
      "source": [
        "Визуализируем случайную игру после обучения алгоритма Value Iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rz4xMaeUiyB2"
      },
      "outputs": [],
      "source": [
        "def play_with_policy(env, policy):\n",
        "  current_state, info = env.reset()\n",
        "  print(env.render())\n",
        "\n",
        "  terminated = False\n",
        "\n",
        "  reward_sum = 0\n",
        "  while not terminated:\n",
        "    next_action = np.argmax(policy[current_state])\n",
        "    time.sleep(0.9)\n",
        "    clear_output(True)\n",
        "    current_state, reward, terminated, truncated, info = env.step(int(next_action))\n",
        "    reward_sum +=reward\n",
        "    print(env.render())\n",
        "    print(f\"Current state: {current_state}\")\n",
        "    print(f\"Truncated: {truncated}\")\n",
        "    print(f\"Terminated: {terminated}\")\n",
        "\n",
        "  print(f\"Total rewards: {reward_sum}\")\n",
        "\n",
        "  # P.S. Если сделать while True, то на финальной точке алгоритм будет подбирать и высаживать пассажира бесконечно:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOHx1ukh1u2P",
        "outputId": "a4d78e71-c479-4578-ae52-28df290c3f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Current state: 475\n",
            "Truncated: False\n",
            "Terminated: True\n",
            "Total rewards: 11\n"
          ]
        }
      ],
      "source": [
        "play_with_policy(env, policy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GW4W18_E9alS"
      },
      "source": [
        "Посчитаем среднюю награду и число побед за 10000 случайных эпизодов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9SkJZK1YjSf1"
      },
      "outputs": [],
      "source": [
        "def evaluate_algorithm(env, policy, episodes=10_000):\n",
        "  rewards = []\n",
        "  wins = 0\n",
        "  for episode in range(episodes):\n",
        "    current_state, info = env.reset()\n",
        "    terminated = False\n",
        "    reward_sum = 0\n",
        "    while not terminated:\n",
        "      next_action = np.argmax(policy[current_state])\n",
        "      current_state, reward, terminated, truncated, info = env.step(int(next_action))\n",
        "      reward_sum +=reward\n",
        "\n",
        "      if terminated and reward == 20:\n",
        "        wins+=1\n",
        "\n",
        "    rewards.append(reward_sum)\n",
        "\n",
        "  print(f\"Количество побед: {wins}\")\n",
        "  print(f\"Средняя награда: {np.mean(rewards)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9bwJOSb6Z-6",
        "outputId": "3e85a555-02c5-4c5e-d3d8-e6c0e2eb7b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество побед: 10000\n",
            "Средняя награда: 7.9066\n"
          ]
        }
      ],
      "source": [
        "evaluate_algorithm(env, policy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W4fQBUSIJXrQ"
      },
      "source": [
        "# Policy Iteration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C-qL8vfwipUl"
      },
      "source": [
        "Реализуем алгоритм Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZTCK5ZvOJa1W"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(environment, gamma=1.0, theta=0.001, max_iter=1000):\n",
        "\n",
        "  n_states = environment.observation_space.n\n",
        "  n_actions = environment.action_space.n\n",
        "\n",
        "  policy = np.ones([n_states, n_actions]) / n_actions\n",
        "\n",
        "  V = np.zeros(environment.observation_space.n)\n",
        "\n",
        "  # Policy Evaluation\n",
        "  for k in range(max_iter):\n",
        "    delta = 0\n",
        "    for state_n in range((n_states)):\n",
        "      v = 0\n",
        "\n",
        "      # Подсчет state-value function (V) для политики policy\n",
        "      for action_n in range((n_actions)):\n",
        "\n",
        "        # policy_s_a - вероятность действия action_n в состоянии state_n под политикой policy\n",
        "        policy_s_a = policy[state_n][action_n]\n",
        "        for probability, next_state, reward, terminated in environment.P[state_n][action_n]:\n",
        "          v  += policy_s_a*probability*(reward + gamma*V[next_state])\n",
        "\n",
        "      delta = max(delta, np.abs(V[state_n] - v))\n",
        "      V[state_n] = v\n",
        "\n",
        "    if delta < theta:\n",
        "      print(f\"Policy evaluation завершено на итерации: {k + 1} \")\n",
        "      break\n",
        "\n",
        "    # Policy improvement\n",
        "    policy_stable = True\n",
        "    for state_n in range((n_states)):\n",
        "\n",
        "      current_best_action = np.argmax(policy[state_n])\n",
        "\n",
        "      # Считаем ценность состояния\n",
        "      action_values = np.zeros(n_actions)\n",
        "      for action_n in range(n_actions):\n",
        "        for probability, next_state, reward, terminated in environment.P[state_n][action_n]:\n",
        "          action_values[action_n] += probability * (reward + gamma* V[next_state])\n",
        "\n",
        "      best_action = np.argmax(action_values)\n",
        "\n",
        "      if current_best_action != best_action:\n",
        "        policy_stable = False\n",
        "        policy[state_n] = np.eye(n_actions)[best_action]\n",
        "\n",
        "    if policy_stable:\n",
        "      print(f\"Лучшая политика была найдена на итерации {k + 1}\")\n",
        "      return policy, V\n",
        "\n",
        "  print(f\"Выполнены все итерации, нет гарантии, что политика стабильна\")\n",
        "  return policy, V"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mJkY5NQ3jyHe"
      },
      "source": [
        "Обучим алгоритм Policy Iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neKwov-qd4ri",
        "outputId": "454ca051-4e25-436e-f263-7cc987afdbd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Лучшая политика была найдена на итерации 18\n"
          ]
        }
      ],
      "source": [
        "environment = gym.make(\"Taxi-v3\")\n",
        "policy, V = policy_iteration(environment)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rsb9qmjWj5_z"
      },
      "source": [
        "Визуализируем случайную игру после обучения алгоритма Policy Iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6ivtyj5jswk",
        "outputId": "4b9853cf-d0d9-42cf-d6b1-5aa2bb769f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Current state: 0\n",
            "Truncated: False\n",
            "Terminated: True\n",
            "Total rewards: 3\n"
          ]
        }
      ],
      "source": [
        "play_with_policy(env, policy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QS3W9a4rkG92"
      },
      "source": [
        "Посчитаем среднюю награду и число побед за 10000 случайных эпизодов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfkUeQvqfHJM",
        "outputId": "079f1bb4-bd13-4e29-c2e4-cd9706d823dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество побед: 10000\n",
            "Средняя награда: 7.5917\n"
          ]
        }
      ],
      "source": [
        "evaluate_algorithm(env, policy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
